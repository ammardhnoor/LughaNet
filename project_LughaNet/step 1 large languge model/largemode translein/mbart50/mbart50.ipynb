{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cda55b8-cd06-4e5d-adf1-e0f59f5f8570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354a94ad-59b8-40b7-95d1-658f9930f04f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba61d2f0-9be1-4c34-a52b-f9d6883f54e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4136cd-5fe1-4716-9709-d1816253ddf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: مرحباً، كيف حالك؟\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "def translate(text, src_lang=\"en_XX\", tgt_lang=\"ar_AR\"):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to the selected device\n",
    "\n",
    "    # Encode the text\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)  # Move tensors to the same device as the model\n",
    "    \n",
    "    # Generate translation\n",
    "    translated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
    "    \n",
    "    # Decode the tokens to string\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example text\n",
    "english_text = \"Hello, how are you?\"\n",
    "# Translate from English to Arabic\n",
    "arabic_translation = translate(english_text)\n",
    "print(\"Translated text:\", arabic_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e43986ce-f701-41f5-a298-7c059d538daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved to 'translated_output.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "def translate(text, src_lang=\"en_XX\", tgt_lang=\"ar_AR\"):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to the selected device\n",
    "\n",
    "    # Encode the text\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)  # Move tensors to the same device as the model\n",
    "    \n",
    "    # Generate translation\n",
    "    translated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
    "    \n",
    "    # Decode the tokens to string\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Load JSON file\n",
    "with open('part2.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Translate the first 10 entries\n",
    "translated_data = []\n",
    "for i in range(min(10, len(data))):\n",
    "    translated_text = translate(data[i])\n",
    "    translated_data.append(translated_text)\n",
    "\n",
    "# Save the translated entries to a new JSON file\n",
    "with open('translated_output.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(translated_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Translation completed and saved to 'translated_output.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87e870ca-6f08-4f5d-9fa0-2f862e116805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated 'autograph album' to 'ألبوم التوثيق'\n",
      "Translated 'lam into' to 'أذهب إلى'\n",
      "Translated 'myeloblastic leukemia' to 'leukemia myeloblastic'\n",
      "Translated 'employment agent' to 'الموظف'\n",
      "Translated 'niger kordofanian' to 'نيجير kordofanian'\n",
      "Translated 'salmon oil' to 'زيت السلمون'\n",
      "Translated 'genus cotinga' to 'genus cotinga'\n",
      "Translated 'gelechia gossypiella' to 'زرقاء الزعانف'\n",
      "Translated 'bower actinidia' to 'بوتر actinidia'\n",
      "Translated 'fish house punch' to 'حذاء منزل الأسماك'\n",
      "Translation completed and saved to 'translated_output.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "def translate(text, src_lang=\"en_XX\", tgt_lang=\"ar_AR\"):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "    # Check if CUDA is available and set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # Move model to the selected device\n",
    "\n",
    "    # Encode the text\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)  # Move tensors to the same device as the model\n",
    "    \n",
    "    # Generate translation\n",
    "    translated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
    "    \n",
    "    # Decode the tokens to string\n",
    "    translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Load JSON file\n",
    "input_file_path = 'part2.json'\n",
    "output_file_path = 'translated_output.json'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Translate the first 10 entries and store pairs\n",
    "translated_data = []\n",
    "for i in range(min(30000, len(data))):\n",
    "    english_word = data[i]\n",
    "    arabic_translation = translate(english_word)\n",
    "    translated_data.append({english_word: arabic_translation})\n",
    "    print(f\"Translated '{english_word}' to '{arabic_translation}'\")  # Debugging print statement\n",
    "\n",
    "# Save the translated entries to a new JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(translated_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Translation completed and saved to '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcae3e2-1072-4e0f-9bc5-0e0ffd6aa9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
