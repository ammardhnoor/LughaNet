{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Orbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Orbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Orbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5757296466973887\n",
      "Recall: 0.7902727017149284\n",
      "F1-Score: 0.6661532081284436\n",
      "Accuracy: 0.6411513723492326\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load JSON data\n",
    "with open('E:/updatetask1/hiontoly/surgury/combinedontology1.json', 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Function to map Arabic words to English\n",
    "def find_english_equivalent(arabic_word, json_data):\n",
    "    for english_word, data in json_data.items():\n",
    "        if \"semantic_relations\" in data and \"synonyms\" in data[\"semantic_relations\"]:\n",
    "            if arabic_word in data[\"semantic_relations\"][\"synonyms\"]:\n",
    "                return english_word\n",
    "    return None\n",
    "\n",
    "def tokenize_arabic(phrase):\n",
    "    return word_tokenize(phrase)\n",
    "\n",
    "def find_english_equivalents(arabic_words, json_data):\n",
    "    english_equivalents = []\n",
    "    for word in arabic_words:\n",
    "        english_word = find_english_equivalent(word, json_data)\n",
    "        if english_word:\n",
    "            english_equivalents.append(english_word)\n",
    "    return english_equivalents\n",
    "\n",
    "# Function to convert tagged words to synsets\n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = {'N': wn.NOUN, 'V': wn.VERB, 'R': wn.ADV, 'J': wn.ADJ}.get(tag[0].upper(), None)\n",
    "    if not wn_tag:\n",
    "        return None\n",
    "    return wn.synsets(word, wn_tag)[0] if wn.synsets(word, wn_tag) else None\n",
    "\n",
    "# Function to calculate sentence similarity\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    "\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0\n",
    "\n",
    "    for synset in synsets1:\n",
    "        best_scores = [synset.path_similarity(ss) for ss in synsets2 if ss]\n",
    "        best_score = max(best_scores) if best_scores else None\n",
    "\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        return 0\n",
    "\n",
    "    score /= count\n",
    "    return score\n",
    "\n",
    "# Lists to store similarity scores\n",
    "similarity_scores_similar = []\n",
    "similarity_scores_not_similar = []\n",
    "\n",
    "# Function to evaluate similarity from a CSV file and store scores\n",
    "def evaluate_similarity_from_csv(csv_file_path, threshold=0.5):\n",
    "    TP = FP = FN = TN = 0\n",
    "    with open(csv_file_path, encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            arabic_phrase1 = row['question1']\n",
    "            arabic_phrase2 = row['question2']\n",
    "            actual_label = int(row['label'])\n",
    "\n",
    "            arabic_words1 = tokenize_arabic(arabic_phrase1)\n",
    "            arabic_words2 = tokenize_arabic(arabic_phrase2)\n",
    "\n",
    "            english_sentence1 = \" \".join(find_english_equivalents(arabic_words1, json_data))\n",
    "            english_sentence2 = \" \".join(find_english_equivalents(arabic_words2, json_data))\n",
    "\n",
    "            similarity_score = sentence_similarity(english_sentence1, english_sentence2)\n",
    "            predicted_label = 1 if similarity_score >= threshold else 0\n",
    "\n",
    "            if predicted_label == actual_label == 1:\n",
    "                TP += 1\n",
    "            elif predicted_label == 1 and actual_label == 0:\n",
    "                FP += 1\n",
    "            elif predicted_label == 0 and actual_label == 1:\n",
    "                FN += 1\n",
    "            elif predicted_label == actual_label == 0:\n",
    "                TN += 1\n",
    "\n",
    "            # Store similarity scores\n",
    "            if actual_label == 1:\n",
    "                similarity_scores_similar.append(similarity_score)\n",
    "            else:\n",
    "                similarity_scores_not_similar.append(similarity_score)\n",
    "\n",
    "            # Stop after processing a certain number of rows\n",
    "            if TP + FP + FN + TN >= 15717:  # Adjust this number as needed\n",
    "                break\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'E:/coding/constructwordnet2/constrcutwordnet2/AREP 12 REALTION/new work bit wordnet and dataset/dataset/up/combined.csv'\n",
    "\n",
    "# Evaluate similarity from CSV\n",
    "precision, recall, f1_score, accuracy = evaluate_similarity_from_csv(csv_file_path)\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
